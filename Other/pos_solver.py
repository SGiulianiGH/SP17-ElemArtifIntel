###################################
# CS B551 Fall 2016, Assignment #3
#
# Your names and user ids:
# Stephen Giuliani (sgiulian)
# Aditya Rajkarne (advrajka)
#
# (Based on skeleton code by D. Crandall)
#
#
####
#
#   -- | WARNING | --
# Running this code took 860 seconds for Part 1.2, 6.7 seconds for Part 1.3, and 713 seconds for Part 2.1!! That's
# almost 17 minutes in total. So if you plan to run it, maybe grab a beer, a shower, order some pizza, or just watch
# Veritasium's YouTube video on Bayes' Theorem while it loads.
#
"""
1.2
-- This took 14.5 minutes to run; not sure why. Using 'bc.test' for training and 'bc.test.tiny' for testing took
-- only took 2.47 seconds....
We have to keep sampling  P(Wn+1 |Wn = wn) to generate words until we hit a period, exclamation point, or question mark.
So we extracted first words in every line, creating a list of all initial words. To sample from it we called
random.choice(), which is similar as traditional sampling as the probability of a word being picked is directly
proportional to the number of times it occurs in the first words' list. Then we accumulate all the words that have been
mentioned after that word including repeats. We call random.choice again to pick a word from the list. This goes on
until the stopping condition is met and we are left with a generated sentence. If we a ';' is chosen, we evaluate if
the prior word in the generated sentence is also a ';'. If we are expecting two ";" in a row, it evaluates the
probability of getting a ;; versus the lowest probability of any otehr word. If the lowest probability of getting any
other word is higher than ;;, then the code randomly selects from a list of all words in the training data. Otherwise,
the ';' will be added.

1.1 and 1.3
Here, we compared the probability of the given sentence under the Markov Chain to the probabilities of the sentences
generated by switching the word to another in the confusion group a brief. First, we check all the words in the sentence
to see which words can they be confused with. We count the number of instances where a sequence(a word following the
word) is also repeated in the training and set.plist stores it. We then calculate probabilities in pfinals using plist
and multiplying all probabilities in pfinals gives the probability of the sentence. We calculate that for all possible
combinations of the sentence that can be formed by replacing words that can be confused.


2.1
-- This took almost 12 minutes to run; not sure why. Using 'bc.test' for training and 'bc.test.tiny' for testing took
-- only took 1.3 seconds....
#######################

                --OUTPUT--
==> So far scored 2000 sentences with 29442 words.
                   Words correct:     Sentences correct:
{'2. HMM VE': 2379, '3. HMM MAP': 26999, '0. Ground truth': 29442, '1. Simplified': 27652}
   0. Ground truth:      100.00%              100.00%
     1. Simplified:       93.92%               47.45%
         2. HMM VE:        8.08%                0.05%
        3. HMM MAP:       91.70%               37.30%
----
--- 712.901000023 seconds ---


  -- | WARNING | --
Running this code took 713 seconds!! That's almost 12 minutes. So if you plan to run it, maybe grab a beer,
a shower, order some pizza, or just watch Veritasium's YouTube video on Bayes' Theorem while it loads.

#####################

Simple Algorithm -
The code takes each word of the sentence and for each, pulls in all possible parts of speech found in the training
file. Then, using the probabilities of that word being each type of POS, the POS with the greatest probability is
kept and added to the output list. Once the max probability POS for each word is added, the algorithm return the
list of POSs. |ASSUMTIONS| : We'd get an error if the word in teh sentence wasn't found in the training data.
This resulted in a '0', which the posterior function would return KeyError on. Therefore, we just assigned the tag
'noun' since 'noun' is the most commonly occuring POS (~18.5% of the training data). The workhorse for this
algorithm was the "pgw" emission calculation, which calculated the probability of a POS, given the word.

HMM VE -
We were unable to build a Variable Elimination algorithm for this assignment. Although all the necessary probabilities
were calculated, we couldn't wrap our heads around execution. We do know that the calculated formula for VE for this
is: P(Wt|St)'Sum-over-all-values-of: St-1' P(St|St-1)P(St-1,W1,W2,...,Wt-1). |WHAT WE DID CALCULATE THOUGH| : In the
essence of time, instead of keeping the 'noun'*len(sentence) output, for fun, we implemented code that randomly
chooses a POS from the set of all POS for each work. So the output is completely random. HOWEVER... We'd like to note
that during a test run of the code (one of millions of runs, it seems), the random POS generator was able to
successfully predict 1 of 2000 sentences (yes, the whole sentence). Of course, we have the picture to prove it.
(it actually happened again on the final run; see the output above)

HMM Viterbi -
The bulk of the training comes into play here where we calculated every possible count and then probability of
words, parts of speech, transitions of both, initials of both, emissions of POS-given-Word and emissions of
Words-given-POS. These were all stored as dictionaries with counts or probabilities (count/total) for access.
The code starts by establishing some needed lists/dictionaries for temporary storage of calculation and then moves
to consider each word of the input sentence. It starts by ensuring the emissions calculations have the provided
word and all potential POS. |ASSUMPTION| : If there was no emission for the word, given a particular POS, we assigned
the emission value of 1/(len(all emissions)+1) so that the assumption is that the combination of that word and POS
was as if it could have been discovered in the very next sentence. Admittedly, this was mostly to avoid a '0' because
'0' was the main culprit of so many errors. The code then stores these calculations in a temporary dictionary to be
considered later for argmax. The code then compares every POS, it's probability, and the probability that it
transitions to a new POS. Then, the code takes the argmax and multiplies it by the emissions probability of all POS
probabilities. Finally, the probabilities within the temp dictionary are all compared until the maximum probability
remains, thus returning that POS as the most likely POS for that word of the sentence (then do that for all words).

Admittedly, our code is clunky and the little comments throughout are either print statements to test as we de-bugged
or just notes as to what/why we were calculating.

"""
####

import random
import math

def do_part12(argv):
    #print "Not implemented yet!"
    import random

    f = open(argv[2])
    raw1 = f.read()
    raw1 = raw1.lower()
    raw1 = raw1.replace("`", "'")
    readlines = raw1.split('\n')
    raw_sent = []
    for i in readlines:
        raw_sent.append(i.split(' '))
    for i in raw_sent:
        while '' in i:
            i.remove('')
    inits=[]
    for i in range(0,len(raw_sent)-1):
        inits.append(raw_sent[i][0])

    ## Creates lists of each sentence and lists of the sentences' parts of speech
    sentences = []
    parts = []
    for i in raw_sent:
        sentences.append(i[::2])
        parts.append(i[1::2])
    sentences.pop() #last element seemed to always be blank
    parts.pop()

    all_words = []
    for i in sentences:
        temp_list = []
        for x in i:
            temp_list.append(x)
        for x in temp_list:     #was this redundant?
            all_words.append(x)

    word_set = list(set(all_words))
    word_prob = []
    for i in word_set:
        word_prob.append([i, float(all_words.count(i)) / float(len(all_words))])
    minw = [1, 1]
    for i in word_prob:
        if i[1] < minw[1]:
            minw = [i[0], i[1]]
    sc_count = 0 #sc = 'semi-colon'
    word_count = 0
    for i in readlines:
        word_count += float(len(i) / 2)
        if i.count(';') > 0:
            sc_count += float(i.count(';') / 2)

    sc_prob = float(sc_count / word_count) #used to determine if ';;' is more likely than ';(anything else)'
    raw_sent.pop()
    for i in range(5):  ## PRINTS 500 FOR TESTING
        next = random.choice(inits)
        sentence = [next]
        while next not in (".", "!", "?"):
            nexts = []
            for i in range(len(raw_sent)):
                if next in raw_sent[i]:
                    try:
                        nexts = nexts + [raw_sent[i][index + 2] for index, word in enumerate(raw_sent[i][:-2]) if
                                         word == next]
                    except IndexError:
                        pass
            if len(nexts) == 0:
                next = random.choice(all_words)
            else:
                next = random.choice(nexts)
            if next == ";":
                if sentence[-1] == ';' and minw > (sc_prob * sc_prob):
                    try:
                        next = random.choice(all_words)
                    except IndexError:
                        pass
                else:
                    if next == ';':
                        continue
            sentence.append(next)

        print(" ".join(sentence))

def do_part13(argv):
    #print "Not implemented yet!"
    import itertools
    usr_input = argv[3]
    usr_input = usr_input.lower().split(' ')
    original = usr_input[:]
    usr_set = set(usr_input)

    # finding subs
    subs = {}  # will store word as a key and a list of confused words as value
    file = open("confused_words.txt", "r")
    lines = file.readlines()

    for i in range(len(lines)):
        lines[i] = lines[i].strip().split(" ")
        for j in range(len(lines[i])):
            if lines[i][j] in usr_set:
                subs[lines[i][j]] = lines[i]
    indices = [i for i in range(len(usr_input)) if
               usr_input[i] in subs]  # gets all indices that represent confused words

    def pcalculator(gen_input):
        gen_set = set(gen_input)
        # get counts
        counter = {}  # stores individual counts
        plist = [0] * (len(gen_input) - 1)  # stores all transition probabilities,p12 stored in index1
        ftrain = open("bc.train", 'r')
        train = ftrain.readlines()
        firsts = []  # stores all first words of the sentence
        for i in range(len(train)):
            train[i] = train[i].strip().lower().split(' ')
            firsts.append(train[i][0])
            for j in range(len(train[i])):
                if train[i][j] in gen_set:
                    if train[i][j] not in counter:
                        counter[train[i][j]] = 1
                    else:
                        counter[train[i][j]] += 1
                    try:
                        if train[i][j + 2] == gen_input[gen_input.index(
                                train[i][j]) + 1]:  # if next word in text is the next word in sentence
                            plist[gen_input.index(train[i][j + 2]) - 1] += 1
                    except IndexError:
                        pass

        pfinals = [float(firsts.count(gen_input[0])) / float(len(firsts))] + [plist[i] / counter[gen_input[i]] for i in
                                                            range(len(plist))]  # po,follwed by transition probabilities
        # print "pfinals: ",pfinals
        z = sorted(set(pfinals))[1] / len(firsts)  # for not found cases min/length
        pmod = [z if pfinals[i] == 0 else pfinals[i] for i in range(len(pfinals))]
        p = 1
        for elem in pmod:
            p *= elem
        return p

    change = []
    for word in usr_input:
        if word in subs:
            change.append(subs[word])
    combo = list(itertools.product(*change))  # generates all possible combinations that can be inserted in the sentence
    suggest = 0
    best = []
    for tups in combo:
        # print(tups)
        for (k, v) in list(zip(indices, tups)):
            usr_input[k] = v
        new = pcalculator(usr_input)
        # print("new",new,best)
        if new > suggest:
            suggest = new
            # print("suggest",suggest)
            best = usr_input[:]
            print"Testing another sentence: ", " ".join(best), " ...with a probability of: ", new
    if best == original:
        print"This seems alright!"
    else:
        print'\nThe original sentence:   "', " ".join(
            original), '"   should be changed. It has a probability of only', pcalculator(usr_input)
        print'Instead, we suggest:   "', " ".join(best), '"   with a probability of', pcalculator(best)



# We've set up a suggested code structure, but feel free to change it. Just
# make sure your code still works with the label.py and pos_scorer.py code
# that we've supplied.
#
class Solver:
    #############################
    # General note: pos = 'Parts of speech'
    t_words_dict = {}
    t_parts_dict = {}
    #t_initial_words_dict = {} #
    t_initial_parts_dict = {}
    #t_transition_words = {} #
    t_transition_parts = {}
    t_wgp_emission = {}
    t_pgw_emission={} #

    #t_words_prob_dict = {} #
    t_parts_prob_dict = {}
    #t_initial_words_prob = {} #
    t_initial_parts_prob = {}
    #t_transition_words_prob = {} #
    t_transition_parts_prob = {}
    #t_wgp_emission_prob = {}    #  ??
    t_pgw_emission_prob={} #
    t_wgp_emission_prob3 = {}

    ##########################

    # Calculate the log of the posterior probability of a given sentence
    #  with a given part-of-speech labeling

    def posterior(self, sentence, label):
        #print "POST - LABEL: ",label
        sum = 0.0
        log_of = math.log(1)
        for i in range(0,len(sentence)):
            if (sentence[i], label[i]) in self.t_wgp_emission_prob3:
                Emission = self.t_wgp_emission_prob3[(sentence[i], label[i])]
            else:
                self.t_wgp_emission_prob3[(sentence[i], label[i])]=float(0.5)
                Emission = 1.000000/100000.00000 #making likelihood very small, not 0 for calc purposes
            if Emission ==0:
                Emission=1.000000/100000.00000 #making likelihood very small, not 0 for calc purposes
            sum += Emission * self.t_parts_prob_dict[label[i]]
            log_of += math.log(float(Emission) * float(self.t_parts_prob_dict[label[i]]))
            posterior_prob = log_of - math.log(sum)
        return posterior_prob
        #return 0

    # Do the training!
    #
    def train(self, data):

        ##################################
        # Building dictionaries -- these are total counts, used for probability
        for i in data:
            # initial first
            # print "Initial w/p"
            # if i[n][0] not in self.t_initial_words_dict:
            # self.t_initial_words_dict[i[n][0]] = float(1)
            # else:
            # self.t_initial_words_dict[i[n][0]] += float(1)
            if i[1][0] not in self.t_initial_parts_dict: #check
                self.t_initial_parts_dict[i[1][0]] = float(1)
            else:
                self.t_initial_parts_dict[i[1][0]] += float(1)
            # print "Transition w/p"
            for x in range(len(i[0])):
                # transition_formula = (i[n][x - 1], i[n][x])
                # Words Dictionary
                if i[0][x] not in self.t_words_dict:
                    self.t_words_dict[i[0][x]] = float(1)
                else:
                    self.t_words_dict[i[0][x]] += float(1)
                # Words Transitions
                # if (i[n][x-1],i[n][x]) not in self.t_transition_words:
                # self.t_transition_words[(i[n][x-1],i[n][x])]= float(1)
                # else:
                # self.t_transition_words[(i[n][x-1],i[n][x])] += float(1)
                # Parts Dictionary
            #for x in range(len(i[1])):
                if i[1][x] not in self.t_parts_dict:
                    self.t_parts_dict[i[1][x]] = float(1)
                else:
                    self.t_parts_dict[i[1][x]] += float(1)
                # Parts Transitions
                if (i[1][x - 1], i[1][x]) not in self.t_transition_parts:
                    self.t_transition_parts[(i[1][x - 1], i[1][x])] = float(1)
                else:
                    self.t_transition_parts[(i[1][x - 1], i[1][x])] += float(1)
            # print "Emission w/p"
            for x in range(len(i[0])):
                # Word-Given-POS Emission
                if (i[0][x], i[1][x]) not in self.t_wgp_emission:
                    self.t_wgp_emission[(i[0][x], i[1][x])] = float(1)
                else:
                    self.t_wgp_emission[(i[0][x], i[1][x])] += float(1)
                # POS-Given-Word Emission
                if (i[1][x],i[0][x]) not in self.t_pgw_emission:
                    self.t_pgw_emission[(i[1][x], i[0][x])] = float(1)
                else:
                    self.t_pgw_emission[(i[1][x], i[0][x])] += float(1)

        # Calculating probabilities for new dictionaries
        # Simple calc for words and pos
        #print "Simple w/p prob"
        # for i in self.t_words_dict.keys():
        # self.t_words_prob_dict[i] = self.t_words_dict[i]/sum(self.t_words_dict.values())
        # print "Simple w: ",self.t_words_prob_dict[i]
        for i in self.t_parts_dict.keys():
            self.t_parts_prob_dict[i] = float(self.t_parts_dict[i]) / float(sum(self.t_parts_dict.values()))
            #print "Simple p: ", self.t_parts_prob_dict[i]
        #print "Initial w/p prob"
        # Initial for words and parts
        # for i in self.t_initial_words_dict.keys():
        # self.t_initial_words_prob[i] = self.t_initial_words_dict[i]/sum(self.t_initial_words_dict.values())
        for i in self.t_initial_parts_dict.keys():
            self.t_initial_parts_prob[i] = float(self.t_initial_parts_dict[i]) / float(sum(self.t_initial_parts_dict.values()))
        #print "Transition w/p prob"
        # Transitions for words and parts
        # for i in self.t_transition_words.keys():
        # self.t_transition_words_prob[i] = self.t_transition_words[i]/sum(self.t_transition_words.values())
        for i in self.t_transition_parts.keys():
            self.t_transition_parts_prob[i] = float(self.t_transition_parts[i]) / float(sum(self.t_transition_parts.values()))
        #print "Emission w/p prob"
        # Emission probs for word-given-part and part-given-word
        # Never got this one below to work, see 't_wgp_emission_prob3'
        for i in self.t_words_dict.keys():
            for x in self.t_parts_dict.keys():
                if (i, x) in self.t_wgp_emission:
                    #self.t_wgp_emission_prob[(i, x)] = self.t_wgp_emission[(i, x)] / self.t_parts_dict[x]
                    self.t_wgp_emission[(i,x)] = float(self.t_wgp_emission[(i, x)]) / float(self.t_parts_dict[x])
        #print "OK, here is the Emission Prob"
        #print self.t_wgp_emission_prob
                    # for i in self.t_pgw_emission.keys():
                    # self.t_pgw_emission_prob[i] = self.t_pgw_emission[i]/sum(self.t_pgw_emission.values())

        #this is the emissions prob calculation that worked after 10 hours of constant testing and editing...
        for i in self.t_wgp_emission:
            self.t_wgp_emission_prob3[i] = float(self.t_wgp_emission[i]) / float(sum(self.t_wgp_emission.values()))
        for i in self.t_pgw_emission:
            self.t_pgw_emission_prob[i] = float(self.t_pgw_emission[i]) / float(sum(self.t_pgw_emission.values()))
        """
        # Calculating probabilities
        def probability_calc_dict(dictionary):
            for i in range(len(dictionary)):
                total = sum(dictionary[i].values())
                for x in dictionary[i].keys():
                    dictionary[i][x] = (dictionary[i][x], dictionary[i][x] / total)
            # THIS WILL REPLACE THE DICTIONARY THAT WAS INPUT
            return dictionary
        print "~~~~~~~~~~~~~~~~~~~~~~END OF ADDED CODE~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
        """
        ################################

    # Functions for each algorithm.
    #
    def simplified(self, sentence):
        #"""
        sent_build = []
        #temp_list=[]
        for i in range(len(sentence)):
            temp_list=[]
            for x in self.t_pgw_emission_prob.keys():
                if sentence[i] in x:
                    temp_list.append([x[0],self.t_pgw_emission_prob[x]])
            max = [0, 0]
            #print "I: ",sentence[i]
            for n in temp_list:
                #print i
                if n[1] > max[1]:
                    max=n
                    #print "N: ",n
            if max[1]==0:
                max=['noun','assumed noun'] ## <--the 'noun' occurs 18.5%, most of all POS, so assumed noun
            sent_build.append(max[0])
        #print "SENT_BUILD: ",sent_build
        return sent_build
        #"""

        #return [ "noun" ] * len(sentence)

    def hmm_ve(self, sentence):
        #return [ "noun" ] * len(sentence)

        ## Haven't developed yet...so RANDOM it is!!...even though choosing noun gets better results...
        parts_list=[]
        for i in self.t_parts_dict.keys():
            parts_list.append(i)
        spitout = []
        for i in range(len(sentence)):
            spitout.append(random.choice(parts_list))
        return spitout


    def hmm_viterbi(self, sentence):
        #return ["noun"] * len(sentence)


        temp_dict = {}
        pos_sentence = {}  # pos_sentence is a sentence made of only POSs
        print_out = []
        # works on initial word
        for POS in self.t_parts_dict.keys():
            if (sentence[0], POS) not in self.t_wgp_emission_prob3:
                self.t_wgp_emission_prob3[(sentence[0], POS)] = 1 / (
                len(self.t_wgp_emission_prob3) + 1)  # as if it would have been the next emission
            P = float(self.t_initial_parts_prob[POS] * self.t_wgp_emission_prob3[(sentence[0], POS)])                                #check this
            temp_dict[(POS, 0)] = P
        # works the rest of the sentence for potential POSs and compares them --NOT LAST WORD IN SENTENCE
        for x in range(1, len(sentence)):
            for potential_pos1 in self.t_parts_dict.keys():
                likely_pos = None
                argmax = float(0)
                for potential_pos2 in self.t_parts_dict.keys():
                    if (potential_pos2, potential_pos1) not in self.t_transition_parts_prob:
                        self.t_transition_parts_prob[(potential_pos2, potential_pos1)]= float(1/(len(self.t_transition_parts_prob)+1))
                    #print "(potential_pos2, x - 1): ",(potential_pos2, x - 1)
                    #print "temp dict: ",temp_dict[(potential_pos2, x - 1)]
                    P = temp_dict[(potential_pos2, x - 1)] * self.t_transition_parts_prob[(potential_pos2, potential_pos1)]
                    if P > argmax:
                        likely_pos = potential_pos2
                        argmax = P
                # if probability is zero then tag 'noun' POS
                if argmax == 0:
                    argmax = 0.001
                    likely_pos = 'noun'  # independent analysis showed noun at 18.5% recurrence, so we assigned
                    # the default value on 'noun' and bumped the max prob to .001 just in case
                if (sentence[x], potential_pos1) not in self.t_wgp_emission_prob3:
                    self.t_wgp_emission_prob3[(sentence[x], potential_pos1)] = float(1 / (len(self.t_wgp_emission_prob3) + 1))
                temp_dict[(potential_pos1, x)] = argmax*self.t_wgp_emission_prob3[(sentence[x], potential_pos1)]
                pos_sentence[(potential_pos1, x)] = likely_pos
        max = float(0)
        sent_len = len(sentence) - 1
        for POS in self.t_parts_dict.keys():  # here's where we calculate the last word
            if temp_dict[(POS, sent_len)] > max:
                likely_pos = POS
                max = temp_dict[(POS, sent_len)]
        print_out.append(likely_pos)
        #sent_len = sent_len+1
        for i in range(sent_len):
            POS = pos_sentence[(likely_pos, sent_len-i)]
            #likely_pos = POS
            print_out.append(POS)

        print_out.reverse()
        return print_out



    # This solve() method is called by label.py, so you should keep the interface the
    #  same, but you can change the code itself. 
    # It's supposed to return a list with two elements:
    #
    #  - The first element is a list of part-of-speech labelings of the sentence.
    #    Each of these is a list, one part of speech per word of the sentence.
    #
    #  - The second element is a list of probabilities, one per word. This is
    #    only needed for simplified() and complex() and is the marginal probability for each word.
    #
    def solve(self, algo, sentence):
        if algo == "Simplified":
            return self.simplified(sentence)
        elif algo == "HMM VE":
            return self.hmm_ve(sentence)
        elif algo == "HMM MAP":
            return self.hmm_viterbi(sentence)
        else:
            print "Unknown algo!"
